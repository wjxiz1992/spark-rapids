/*
 * Copyright (c) 2022, NVIDIA CORPORATION.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.nvidia.spark.rapids

import org.apache.hadoop.fs.Path

import org.apache.spark.sql.catalyst.expressions.Expression
import org.apache.spark.sql.execution.datasources.{CatalogFileIndex, FileIndex, HadoopFsRelation, InMemoryFileIndex, PartitioningAwareFileIndex, PartitionSpec}

object AlluxioUtils {
  def replacePathIfNeeded(
                           conf: RapidsConf,
                           relation: HadoopFsRelation,
                           partitionFilters: Seq[Expression],
                           dataFilters: Seq[Expression]): FileIndex = {

    val alluxioPathsReplace: Option[Seq[String]] = conf.getAlluxioPathsToReplace

    if (alluxioPathsReplace.isDefined) {
      // alluxioPathsReplace: Seq("key->value", "key1->value1")
      // turn the rules to the Map with eg
      // { s3:/foo -> alluxio://0.1.2.3:19998/foo,
      //   gs:/bar -> alluxio://0.1.2.3:19998/bar,
      //   /baz -> alluxio://0.1.2.3:19998/baz }
      val replaceMapOption = alluxioPathsReplace.map(rules => {
        rules.map(rule => {
          val split = rule.split("->")
          if (split.size == 2) {
            split(0).trim -> split(1).trim
          } else {
            throw new IllegalArgumentException(s"Invalid setting for " +
              s"${RapidsConf.ALLUXIO_PATHS_REPLACE.key}")
          }
        }).toMap
      })

      replaceMapOption.map(replaceMap => {
        // replacement func to check if the file path is prefixed with the string user configured
        // if yes, replace it
        val replaceFunc = (f: Path) => {
          val pathStr = f.toString
          val matchedSet = replaceMap.keySet.filter(reg => pathStr.startsWith(reg))
          if (matchedSet.size > 1) {
            // never reach here since replaceMap is a Map
            throw new IllegalArgumentException(s"Found ${matchedSet.size} same replacing rules " +
              s"from ${RapidsConf.ALLUXIO_PATHS_REPLACE.key} which requires only 1 rule " +
              s"for each file path")
          } else if (matchedSet.size == 1) {
            new Path(pathStr.replaceFirst(matchedSet.head, replaceMap(matchedSet.head)))
          } else {
            f
          }
        }

        val parameters: Map[String, String] = relation.options

        if (relation.location.isInstanceOf[PartitioningAwareFileIndex]) {
          val fi = relation.location.asInstanceOf[PartitioningAwareFileIndex]
          val spec = fi.partitionSpec()
          val partitionsReplaced = spec.partitions.map { p =>
            val replacedPath = replaceFunc(p.path)
            org.apache.spark.sql.execution.datasources.PartitionPath(p.values, replacedPath)
          }
          val specAdjusted = PartitionSpec(spec.partitionColumns, partitionsReplaced)
          val replacedPaths = fi.rootPaths.map { p => replaceFunc(p) }
          new InMemoryFileIndex(
            relation.sparkSession,
            replacedPaths,
            parameters,
            Option(relation.dataSchema),
            userSpecifiedPartitionSpec = Some(specAdjusted))
        } else if (relation.location.isInstanceOf[CatalogFileIndex]) {
          val fi = relation.location.asInstanceOf[CatalogFileIndex]
          val memFI = fi.filterPartitions(Nil)
          val spec = memFI.partitionSpec()
          val partitionsReplaced = spec.partitions.map { p =>
            val replacedPath = replaceFunc(p.path)
            org.apache.spark.sql.execution.datasources.PartitionPath(p.values, replacedPath)
          }
          val replacedPaths = memFI.rootPaths.map { p => replaceFunc(p) }
          val specAdjusted = PartitionSpec(spec.partitionColumns, partitionsReplaced)
          new InMemoryFileIndex(
            relation.sparkSession,
            replacedPaths,
            parameters,
            Option(relation.dataSchema),
            userSpecifiedPartitionSpec = Some(specAdjusted))
        } else {
          relation.location
        }
      }).getOrElse(relation.location)
    } else {
      relation.location
    }
  }
}
